{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I06WcIQ7JRFy"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialise SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "import json, re\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "            .appName(\"WordCount\") \\\n",
        "            .master(\"local[*]\") \\\n",
        "            .getOrCreate()\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "N3sODa1GNwt3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Creating input files\n",
        "# Word Count input\n",
        "with open(\"input.txt\", \"w\") as f:\n",
        "    f.write(\"hello world\\nhello spark\\nhello pyspark world\")\n",
        "\n",
        "# CSV files (users & purchases)\n",
        "with open(\"users.csv\", \"w\") as f:\n",
        "    f.write(\"user_id,name\\n1,Alice\\n2,Bob\\n3,Charlie\")\n",
        "\n",
        "with open(\"purchases.csv\", \"w\") as f:\n",
        "    f.write(\"user_id,product,amount\\n1,Book,100.0\\n2,Laptop,800.5\\n1,Pen,50.0\\n3,Phone,200.0\")\n",
        "\n",
        "# JSON file\n",
        "with open(\"people.json\", \"w\") as f:\n",
        "    f.write('{\"name\": \"Alice\", \"age\": 30}\\n')\n",
        "    f.write('{\"name\": \"Bob\", \"age\": 40}\\n')\n",
        "    f.write('{\"name\": \"Eve\", \"age\": 29}\\n')\n",
        "    f.write('{\"name\": \"Charlie\", \"age\": 40}\\n')\n",
        "\n",
        "# Event file\n",
        "with open(\"events.jsonl\", \"w\") as f:\n",
        "    f.write('{\"event\": \"login\", \"user\": \"Alice\"}\\n')\n",
        "    f.write('{\"event\": \"purchase\", \"user\": \"Bob\", \"amount\": 100.0}\\n')\n",
        "    f.write('{\"event\": \"login\", \"user\": \"Charlie\"}\\n')\n",
        "    f.write('{\"event\": \"purchase\", \"user\": \"Alice\", \"amount\": 50.0}\\n')\n",
        "    f.write('{\"event\": \"login\", \"user\": \"Bob\"}\\n')\n",
        "    f.write('{\"event\": \"purchase\", \"user\": \"Charlie\", \"amount\": 200.0}\\n')\n",
        "    f.write('{\"event\": \"purchase\", \"user\": \"Alice\", \"amount\": 300.0}\\n')\n",
        "    f.write('{\"event\": \"login\", \"user\": \"Charlie\"}\\n')\n",
        "    f.write('{\"event\": \"purchase\", \"user\": \"Bob\", \"amount\": 400.0}\\n')\n",
        "    f.write('{\"event\": \"login\", \"user\": \"Alice\"}\\n')\n",
        "\n",
        "# Apache log file\n",
        "with open(\"apache.access.log\", \"w\") as f:\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:55:36 -0700] \"GET /index.html HTTP/1.0\" 200 2326\\n')\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:55:56 -0700] \"GET /products HTTP/1.0\" 200 1234\\n')\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:56:01 -0700] \"GET /index.html HTTP/1.0\" 200 2326\\n')\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:56:16 -0700] \"GET /login HTTP/1.0\" 200 543\\n')\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:56:36 -0700] \"GET /index.html HTTP/1.0\" 200 2326\\n')\n",
        "    f.write('127.0.0.1 - - [10/Oct/2000:13:56:56 -0700] \"GET /products HTTP/1.0\" 200 1234\\n')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L8bhki3wSy3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6.1 - Word Count (DataFrame / SQL)\n",
        "print(\"\\n=== Word Count (DataFrame / SQL) ===\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, split, lower, col\n",
        "import sys\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exp7_WordCount_SQL\").getOrCreate()\n",
        "input_path = \"./input.txt\"\n",
        "\n",
        "df = spark.read.text(input_path)\n",
        "words = df.select(explode(split(col(\"value\"), \"\\\\s+\")).alias(\"word\"))\n",
        "words = words.withColumn(\"word\", lower(col(\"word\"))).filter(col(\"word\") != \"\")\n",
        "counts = words.groupBy(\"word\").count().orderBy(col(\"count\").desc())\n",
        "counts.show(50, truncate=False)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "M0uaWoPYWaDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6.2 â€” CSV Join & Aggregation (DataFrame / SQL)\n",
        "print(\"\\n=== CSV Join & Aggregation (DataFrame / SQL) ===\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "import sys\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exp7_Join_SQL\").getOrCreate()\n",
        "people_path = \"./users.csv\"\n",
        "purchases_path = \"./purchases.csv\"\n",
        "\n",
        "people = spark.read.csv(people_path, header=True, inferSchema=True)\n",
        "purchases = spark.read.csv(purchases_path, header=True, inferSchema=True)\n",
        "\n",
        "joined = people.join(purchases, on=\"user_id\", how=\"inner\")\n",
        "result = joined.groupBy(\"name\").agg(_sum(\"amount\").alias(\"total_amount\")).orderBy(\"name\")\n",
        "result.show(truncate=False)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "DLGHtt6UWgQq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6.3 - JSONL processing (DataFrame)\n",
        "print(\"\\n=== JSON Processing (DataFrame) ===\")\n",
        "from pyspark.sql import SparkSession\n",
        "import sys\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exp7_Events_SQL\").getOrCreate()\n",
        "path = \"./events.jsonl\"\n",
        "\n",
        "df = spark.read.json(path)\n",
        "df.groupBy(\"event\").count().show()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4Wh3_pmZWmjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 6.4 - Apache Log parsing (DataFrame + SQL)\n",
        "print(\"\\n=== Apache Log Parsing (DataFrame + SQL) ===\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import regexp_extract, col\n",
        "import sys\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exp7_Apache_SQL\").getOrCreate()\n",
        "path = \"./apache.access.log\"\n",
        "\n",
        "df = spark.read.text(path)\n",
        "# regex to extract request part and then path\n",
        "# group 1: ip, group 2: datetime, group 3: request, group 4: status, group 5: size\n",
        "log_regex = r'(\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\S+)'\n",
        "df2 = df.withColumn(\"request\", regexp_extract(col(\"value\"), log_regex, 3))\n",
        "# extract path from request (split by space)\n",
        "from pyspark.sql.functions import split\n",
        "df3 = df2.withColumn(\"path\", split(col(\"request\"), \" \").getItem(1))\n",
        "df3.groupBy(\"path\").count().orderBy(col(\"count\").desc()).show(truncate=False)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eTvdWDc7WtPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Modify the join to use broadcast join in SQL for small people.csv and compare shuffle metrics (spark.sql.autoBroadcastJoinThreshold).\n",
        "print(\"\\n=== Modified CSV Join & Aggregation (DataFrame / SQL) ===\")\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum as _sum\n",
        "import sys\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Exp7_Join_SQL\").getOrCreate()\n",
        "\n",
        "# Set the autoBroadcastJoinThreshold for broadcast join\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10*1024*1024) # Set to 10MB\n",
        "\n",
        "people_path = \"./users.csv\"\n",
        "purchases_path = \"./purchases.csv\"\n",
        "\n",
        "people = spark.read.csv(people_path, header=True, inferSchema=True)\n",
        "purchases = spark.read.csv(purchases_path, header=True, inferSchema=True)\n",
        "\n",
        "joined = people.join(purchases, on=\"user_id\", how=\"inner\")\n",
        "result = joined.groupBy(\"name\").agg(_sum(\"amount\").alias(\"total_amount\")).orderBy(\"name\")\n",
        "result.show(truncate=False)\n",
        "\n",
        "# To compare shuffle metrics, you would typically need to inspect the Spark UI.\n",
        "# However, programmatically we can try to trigger the execution and see the execution plan.\n",
        "print(\"\\n=== Execution Plan ===\")\n",
        "result.explain()\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vIbUP9ybzz-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}