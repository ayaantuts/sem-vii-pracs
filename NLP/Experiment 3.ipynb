{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0491c2ac",
   "metadata": {},
   "source": [
    "Apply these steps on a small regional language dataset\n",
    "1. Implement & analyze word level analysis using BiGram & Tri-gram models\n",
    "2. Evaluate the performance of the models in terms of accuracy & coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94bc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sentence (with smoothing): नैसर्गिक प्रक्रिया मानवी समजावून क्षमता .\n",
      "Perplexity for Bi-gram Model (with smoothing): 1.0\n",
      "Perplexity for Tri-gram Model (with smoothing): 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Step 1: Download required NLTK resources (if you haven't already)\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Sample Marathi text\n",
    "text = \"नैसर्गिक भाषा प्रक्रिया संगणकांना मानवी भाषा समजावून देण्याची क्षमता देते.\"\n",
    "\n",
    "# Step 2: Tokenize the text (Word Tokenization)\n",
    "tokens = word_tokenize(text.lower())  # Convert to lowercase for consistency\n",
    "\n",
    "# Step 3: Generate Bi-grams and Tri-grams\n",
    "bigrams = ngrams(tokens, 2)\n",
    "trigrams = ngrams(tokens, 3)\n",
    "\n",
    "# Step 4: Count Bi-grams and Tri-grams\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Step 5: Additive Smoothing for Probabilities\n",
    "def bigram_prob_smooth(bigram, bigram_counts, vocab_size):\n",
    "    # Laplace smoothing: P(w2|w1) = (Count(w1, w2) + 1) / (Count(w1) + vocab_size)\n",
    "    return (bigram_counts[bigram] + 1) / (sum(bigram_counts.values()) + vocab_size)\n",
    "\n",
    "def trigram_prob_smooth(trigram, trigram_counts, bigram_counts, vocab_size):\n",
    "    # Laplace smoothing for trigram: P(w3|w1,w2) = (Count(w1,w2,w3) + 1) / (Count(w1,w2) + vocab_size)\n",
    "    return (trigram_counts[trigram] + 1) / (bigram_counts[(trigram[0], trigram[1])] + vocab_size)\n",
    "\n",
    "# Vocabulary size (total unique words)\n",
    "vocab_size = len(set(tokens))\n",
    "\n",
    "# Step 6: Generate a Random Sentence from Tri-gram Model with smoothing\n",
    "def generate_sentence_smooth(trigram_model, bigram_model, trigram_counts, bigram_counts, vocab_size):\n",
    "    sentence = [tokens[0]]  # Start with the first word\n",
    "    while len(sentence) < 10:  # Limit to 10 words for simplicity\n",
    "        next_word_candidates = [\n",
    "            trigram for trigram in trigram_counts if trigram[0] == sentence[-1]\n",
    "        ]\n",
    "        if next_word_candidates:\n",
    "            next_word = random.choice(next_word_candidates)[2]\n",
    "            sentence.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "# Generate a sentence from trigram model with smoothing\n",
    "generated_sentence = generate_sentence_smooth(trigrams, bigrams, trigram_counts, bigram_counts, vocab_size)\n",
    "print(f\"Generated Sentence (with smoothing): {generated_sentence}\")\n",
    "\n",
    "# Step 7: Calculate Perplexity with Smoothing for Bi-gram and Tri-gram models\n",
    "def calculate_perplexity_smooth(ngrams, ngram_counts, ngram_prob_func, vocab_size):\n",
    "    log_likelihood = 0\n",
    "    total_ngrams = sum(ngram_counts.values())  # Total count of all n-grams\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        try:\n",
    "            log_likelihood += math.log2(ngram_prob_func(ngram, ngram_counts, vocab_size))  # Log probability for each n-gram\n",
    "        except KeyError:\n",
    "            # If n-gram doesn't exist in the model, handle unseen n-grams\n",
    "            log_likelihood += math.log2(1 / (total_ngrams + vocab_size))  # Assume unseen n-grams have a small probability\n",
    "    perplexity = 2 ** (-log_likelihood / total_ngrams)  # Calculate perplexity\n",
    "    return perplexity\n",
    "\n",
    "# Calculate Perplexity for Bi-gram and Tri-gram models with smoothing\n",
    "bigram_perplexity = calculate_perplexity_smooth(bigrams, bigram_counts, bigram_prob_smooth, vocab_size)\n",
    "trigram_perplexity = calculate_perplexity_smooth(trigrams, trigram_counts, trigram_prob_smooth, vocab_size)\n",
    "\n",
    "print(f\"Perplexity for Bi-gram Model (with smoothing): {bigram_perplexity}\")\n",
    "print(f\"Perplexity for Tri-gram Model (with smoothing): {trigram_perplexity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
